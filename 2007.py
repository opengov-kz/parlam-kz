import os
import time
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
from tqdm import tqdm

# –ë–∞–∑–æ–≤–∞—è —Å—Å—ã–ª–∫–∞ –∞—Ä—Ö–∏–≤–∞ 2007
BASE_URL = "https://web.archive.org/web/20070815000000/http://www.parlam.kz/"
WAYBACK_PREFIX = "https://web.archive.org"

# –ü–∞–ø–∫–∏ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
SAVE_HTML = "saved_2007/html"
SAVE_DOCS = "saved_2007/docs"
os.makedirs(SAVE_HTML, exist_ok=True)
os.makedirs(SAVE_DOCS, exist_ok=True)

# –¶–µ–ª–µ–≤—ã–µ —Ç–∏–ø—ã —Ñ–∞–π–ª–æ–≤
DOCUMENT_EXTENSIONS = [".pdf", ".doc", ".docx", ".xls", ".xlsx", ".rtf", ".zip", ".rar", ".txt"]
IGNORED_EXTENSIONS = [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".svg", ".ico", ".css", ".js"]

# –ü—Ä–æ–≤–µ—Ä–∫–∞ ‚Äî –¥–æ–∫—É–º–µ–Ω—Ç –ª–∏
def is_document(url):
    return any(url.lower().endswith(ext) for ext in DOCUMENT_EXTENSIONS)

def is_ignored(url):
    return any(url.lower().endswith(ext) for ext in IGNORED_EXTENSIONS)

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
def download_file(url, folder):
    try:
        name = os.path.basename(urlparse(url).path)
        if not name or '.' not in name:
            return
        path = os.path.join(folder, name)
        if os.path.exists(path):
            return
        r = requests.get(url, stream=True, timeout=15)
        with open(path, 'wb') as f:
            for chunk in r.iter_content(chunk_size=8192):
                f.write(chunk)
        print(f"üìÑ –§–∞–π–ª —Å–∫–∞—á–∞–Ω: {name}")
    except Exception as e:
        print(f"‚ö† –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞: {url} ‚Äî {e}")

# –°–∫–∞—á–∏–≤–∞–Ω–∏–µ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü
def download_html(url):
    try:
        name = url.replace("/", "_").replace(":", "").replace("?", "")
        name = name[-100:]  # –æ–≥—Ä–∞–Ω–∏—á–∏–º –¥–ª–∏–Ω—É
        path = os.path.join(SAVE_HTML, name + ".html")
        if os.path.exists(path):
            return None
        r = requests.get(url, timeout=15)
        with open(path, "w", encoding="utf-8") as f:
            f.write(r.text)
        print(f"üåê –°—Ç—Ä–∞–Ω–∏—Ü–∞ —Å–∫–∞—á–∞–Ω–∞: {url}")
        return r.text
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ HTML: {url} ‚Äî {e}")
        return None

# –û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ö–æ–¥
def crawl(start_url, sleep_between_requests=1.0):
    visited = set()
    to_visit = [start_url]
    pages = 0
    docs = 0

    while to_visit:
        current = to_visit.pop()
        if current in visited or not current.startswith(WAYBACK_PREFIX):
            continue
        visited.add(current)

        print(f"üîé –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {current}")
        html = download_html(current)
        if not html:
            continue
        pages += 1
        time.sleep(sleep_between_requests)

        soup = BeautifulSoup(html, "html.parser")
        for a in soup.find_all("a", href=True):
            href = a["href"]
            full_url = urljoin(current, href)

            if is_ignored(full_url):
                continue
            if is_document(full_url):
                download_file(full_url, SAVE_DOCS)
                docs += 1
            elif "parlam.kz" in full_url and full_url not in visited and full_url.startswith(WAYBACK_PREFIX):
                to_visit.append(full_url)

    print(f"\n‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ: —Å–∫–∞—á–∞–Ω–æ {pages} —Å—Ç—Ä–∞–Ω–∏—Ü –∏ {docs} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

# üöÄ –°—Ç–∞—Ä—Ç
if __name__ == "__main__":
    crawl(BASE_URL)
